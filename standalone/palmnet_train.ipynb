{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 3584\n",
      "Eval files: 1615\n",
      "Labels: 300\n"
     ]
    }
   ],
   "source": [
    "IMAGES_DIR = os.path.join(\"..\", \"datasets\", \"CASIA-PalmprintV1\")\n",
    "num_classes = 300\n",
    "\n",
    "def train_test_split():\n",
    "    labels = sorted(os.listdir(IMAGES_DIR))[:num_classes]\n",
    "    train_files = []\n",
    "    eval_files = []\n",
    "    for label in labels:\n",
    "        folder = os.path.join(IMAGES_DIR, label)\n",
    "        files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith(\".jpg\")]\n",
    "        shuffle(files)\n",
    "        split_idx = int(len(files) * 0.7)\n",
    "        train_files.extend(files[:split_idx])\n",
    "        eval_files.extend(files[split_idx:])\n",
    "    return train_files, eval_files, labels\n",
    "\n",
    "train_files, eval_files, labels = train_test_split()\n",
    "print(\"Train files: {}\".format(len(train_files)))\n",
    "print(\"Eval files: {}\".format(len(eval_files)))\n",
    "print(\"Labels: {}\".format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5199/5199 [00:14<00:00, 354.31it/s]\n"
     ]
    }
   ],
   "source": [
    "image_by_filename = {}\n",
    "files = train_files + eval_files\n",
    "for file in tqdm(files):\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.resize(image, (224, 224)).astype(np.float32)\n",
    "    image /= 255.\n",
    "    image_by_filename[file] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonIDSequence(Sequence):\n",
    "\n",
    "    def __init__(self, files, labels, batch_size, extract_palm=False):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        shuffle(self.files)\n",
    "        self.num_labels = len(self.labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.extract_palm = extract_palm\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.files) / self.batch_size)\n",
    "\n",
    "    def load_palm_print(self, image_file):\n",
    "        if INPUT_SHAPE[2] == 1:\n",
    "            image = cv2.imread(image_file, 0)\n",
    "        else:\n",
    "            image = cv2.imread(image_file)\n",
    "        if self.extract_palm:\n",
    "            image = extract_palm_from_img(image)\n",
    "        try:\n",
    "            image = cv2.resize(image, INPUT_SHAPE[:2])\n",
    "        except:\n",
    "            print(\"image_file:\", image_file)\n",
    "        image = image * 1./255\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        y = [os.path.basename(os.path.dirname(file)) for file in X ]\n",
    "        palm_prints = np.array([image_by_filename[image] for image in X])\n",
    "        y_indices = [to_categorical(self.labels.index(i), num_classes=self.num_labels)\n",
    "                     for i in y]\n",
    "        return palm_prints, np.array(y_indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "def palm_model():\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=INPUT_SHAPE)\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    x = vgg16.output\n",
    "    x = layers.Flatten()(x)                                # Flatten dimensions to for use in FC layers\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)                             # Dropout layer to reduce overfitting\n",
    "    x = layers.Dense(len(labels), name=\"last_dense\")(x) \n",
    "    x = layers.Softmax()(x)                                # Softmax for multiclass\n",
    "    return Model(inputs=vgg16.input, outputs=x)\n",
    "\n",
    "model = palm_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PersonIDSequence(train_files, labels, batch_size=64)\n",
    "eval_ds = PersonIDSequence(eval_files, labels, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 56 steps, validate for 26 steps\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 30s 541ms/step - loss: 5.6622 - accuracy: 0.0184 - val_loss: 5.1318 - val_accuracy: 0.0811\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 23s 419ms/step - loss: 4.6190 - accuracy: 0.1183 - val_loss: 4.0483 - val_accuracy: 0.2502\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 24s 422ms/step - loss: 3.3119 - accuracy: 0.3354 - val_loss: 2.8988 - val_accuracy: 0.4019\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 24s 425ms/step - loss: 2.1373 - accuracy: 0.5711 - val_loss: 1.9518 - val_accuracy: 0.6378\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 24s 434ms/step - loss: 1.3024 - accuracy: 0.7600 - val_loss: 1.3481 - val_accuracy: 0.7666\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 26s 464ms/step - loss: 0.8435 - accuracy: 0.8594 - val_loss: 0.9575 - val_accuracy: 0.8427\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 26s 471ms/step - loss: 0.5162 - accuracy: 0.9283 - val_loss: 0.6703 - val_accuracy: 0.8885\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 27s 473ms/step - loss: 0.3199 - accuracy: 0.9629 - val_loss: 0.5535 - val_accuracy: 0.8954\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 27s 473ms/step - loss: 0.2328 - accuracy: 0.9799 - val_loss: 0.4418 - val_accuracy: 0.9251\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 26s 473ms/step - loss: 0.1659 - accuracy: 0.9880 - val_loss: 0.3701 - val_accuracy: 0.9331\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.0001\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(lr))\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 7s 268ms/step - loss: 0.3701 - accuracy: 0.9331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37008239386173397, 0.9331269]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"palm_model_e{}_lr{}.h5\".format(epochs, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
