{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://github.com/kairess/fingerprint_recognition/blob/master/train.ipynb\n",
    "\n",
    "def palm_model():\n",
    "    x1 = layers.Input(shape=(90, 90, 1))\n",
    "    x2 = layers.Input(shape=(90, 90, 1))\n",
    "\n",
    "    # share weights both inputs\n",
    "    inputs = layers.Input(shape=(90, 90, 1))\n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "\n",
    "    feature_model = Model(inputs=inputs, outputs=x, name='feature_model')\n",
    "    \n",
    "    x1_net = feature_model(x1)\n",
    "    x2_net = feature_model(x2)\n",
    "    \n",
    "    # subtract features\n",
    "    net = layers.Subtract(name='subtract')([x1_net, x2_net])\n",
    "\n",
    "    net = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', name='diff_conv_net')(net)\n",
    "    net = layers.MaxPooling2D(pool_size=2, name='maxpool_net')(net)\n",
    "    net = layers.Flatten(name='flatten_net')(net)\n",
    "    net = layers.Dropout(0.5)(net)\n",
    "    net = layers.Dense(64, activation='relu', name='relu_net')(net)\n",
    "    net = layers.Dropout(0.5)(net)\n",
    "    net = layers.Dense(1, activation='sigmoid', name='sigmoid_net')(net)\n",
    "    model = Model(inputs=[x1, x2], outputs=net)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(start_idx, end_idx, batch_size=128, is_training=False):\n",
    "    train_df = pd.read_csv(\"datasets/train.csv\", usecols=['palm_print', 'label'])\n",
    "    val_df = pd.read_csv(\"datasets/val.csv\", usecols=['palm_print', 'label'])\n",
    "    df = pd.concat([train_df, val_df])\n",
    "    df = df[df.label.isin(list(range(start_idx, end_idx)))]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df.join(df, lsuffix=\"_left\", rsuffix=\"_right\", how='cross')\n",
    "    df = df[df.palm_print_left != df.palm_print_right]\n",
    "    pos_df = df[df.label_left == df.label_right]\n",
    "    neg_df = df[df.label_left != df.label_right]\n",
    "    neg_df = neg_df.sample(n=min(pos_df.shape[0], neg_df.shape[0]))\n",
    "    neg_df = neg_df.sample(n=min(pos_df.shape[0], neg_df.shape[0]))\n",
    "    balanced_df = pd.concat([pos_df, neg_df]).sample(frac=1)\n",
    "    print(\"Total no. of records:\", balanced_df.shape[0])\n",
    "    print(\"No. of similar pairs:\", balanced_df[balanced_df.label_left == balanced_df.label_right].shape[0])\n",
    "    print(\"No. of distin. pairs:\", balanced_df[balanced_df.label_left != balanced_df.label_right].shape[0])\n",
    "    print(\"No. of batches:\", balanced_df.shape[0]//batch_size)\n",
    "    def process(dataframe):\n",
    "        def read_img():\n",
    "            for idx, row in dataframe.iterrows():\n",
    "                left_img = cv2.resize(cv2.imread(row.palm_print_left, 0), (90, 90))/255.\n",
    "                right_img = cv2.resize(cv2.imread(row.palm_print_right, 0), (90, 90))/255.\n",
    "                label = 0 if row.label_left == row.label_right else 1\n",
    "                yield (np.expand_dims(left_img, axis=-1), np.expand_dims(right_img, axis=-1)), label\n",
    "        return read_img\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(process(balanced_df), output_types=((np.float32, np.float32), np.int8),\n",
    "                                            output_shapes=(((90, 90, 1), (90, 90, 1)), []))\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat().shuffle(buffer_size=7000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in get_dataset(0, 150, batch_size=128).take(1):\n",
    "    print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataset = get_dataset(0, 150, batch_size=batch_size, is_training=True)\n",
    "val_dataset = get_dataset(150, 200, batch_size=batch_size)\n",
    "test_dataset = get_dataset(200, 210, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = palm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = 50\n",
    "validation_steps = 3\n",
    "\n",
    "history = model.fit(train_dataset, epochs=epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=test_dataset, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model = Model(model.get_layer('feature_model').input, outputs=model.get_layer('feature_model').output)\n",
    "# features_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model = Sequential()\n",
    "diff_model.add(layers.Input(shape=model.get_layer(\"diff_conv_net\").input.shape[1:]))\n",
    "diff_model.add(model.get_layer(\"diff_conv_net\"))\n",
    "diff_model.add(model.get_layer(\"maxpool_net\"))\n",
    "diff_model.add(model.get_layer(\"flatten_net\"))\n",
    "diff_model.add(model.get_layer(\"relu_net\"))\n",
    "diff_model.add(model.get_layer(\"sigmoid_net\"))\n",
    "diff_model = Model(inputs=diff_model.inputs, outputs=diff_model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model.save(\"models/palm_features_model.h5\")\n",
    "diff_model.save(\"models/palm_diff_model.h5\")\n",
    "model.save(\"models/palm_siamese_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/palm_siamese_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset = get_dataset(0, 200, batch_size=128)\n",
    "# model.evaluate(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(250, 252, batch_size=128)\n",
    "model.evaluate(test_dataset)\n",
    "model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = \"datasets/PalmCropped/100/0100_m_l_03.jpg\"\n",
    "file_2 = \"datasets/PalmCropped/101/0101_m_l_04.jpg\"\n",
    "\n",
    "def preproc(filename):\n",
    "    img = np.expand_dims(cv2.resize(cv2.imread(file_1, 0), (90, 90)), (0, -1))\n",
    "    return img\n",
    "\n",
    "img_1 = preproc(file_1)\n",
    "img_2 = preproc(file_2)\n",
    "\n",
    "feat_1 = features_model.predict(img_1)\n",
    "feat_2 = features_model.predict(img_2)\n",
    "sub_feat = feat_2 - feat_1\n",
    "prediction = diff_model.predict(sub_feat)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_feat = feat_1 - feat_2\n",
    "prediction = diff_model.predict(sub_feat)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_utils import face_model\n",
    "from preproc_utils import face_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from scipy.spatial.distance import cosine\n",
    "from PIL import Image\n",
    "\n",
    "# Face\n",
    "def face_preprocess(file, size=(224, 224)):\n",
    "    image = cv2.imread(file)[::-1]\n",
    "    image = cv2.resize(image, size)\n",
    "    face = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "    face = preprocess_input(face, version=2)\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(model, file):\n",
    "    face = face_preprocess(file, (224, 224))\n",
    "    yhat = model.predict(face)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = \"datasets/PalmCropped/100/0100_m_l_03.jpg\"\n",
    "file_2 = \"datasets/PalmCropped/100/0100_m_l_04.jpg\"\n",
    "file_3 = \"datasets/PalmCropped/101/0101_m_l_04.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = face_model()\n",
    "enc_1 = get_encodings(model, file_1)\n",
    "enc_2 = get_encodings(model, file_2)\n",
    "enc_3 = get_encodings(model, file_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007586956024169922\n",
      "0.07879728078842163\n",
      "0.06955486536026001\n"
     ]
    }
   ],
   "source": [
    "print(cosine(enc_1, enc_2))\n",
    "print(cosine(enc_1, enc_3))\n",
    "print(cosine(enc_2, enc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = sorted(list(glob(\"datasets/PalmCropped/*/*.jpg\")), key=lambda x: os.path.basename(x))\n",
    "files = files[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0\n",
      "True 0\n",
      "False 0\n",
      "True 0\n",
      "True 1\n",
      "False 0\n",
      "False 0\n",
      "True 0\n",
      "False 0\n",
      "True 1\n",
      "False 0\n",
      "False 0\n",
      "True 0\n",
      "True 0\n",
      "True 1\n"
     ]
    }
   ],
   "source": [
    "for file_1, file_2 in zip(files, files[1:]):\n",
    "    enc_1 = get_encodings(model, file_1)\n",
    "    enc_2 = get_encodings(model, file_2)\n",
    "    print(cosine(enc_1, enc_2)>0.01, 0 if os.path.basename(os.path.dirname(file_1)) ==\n",
    "                                      os.path.basename(os.path.dirname(file_2)) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
